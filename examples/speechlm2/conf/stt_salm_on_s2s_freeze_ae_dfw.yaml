model:
  # Every name/path here starting with 'pretrained' is used to initialize the model weights.
  pretrained_llm: TinyLlama/TinyLlama_v1.1
  # pretrained_asr: nvidia/canary-1b-flash
  pretrained_asr: /lustre/fsw/portfolios/convai/users/cchen1/duplex_s2s/stt_en_fastconformer_hybrid_large_streaming_80ms.nemo

  pretrained_weights: True  # When False, we use pretrained_name to load the architecture, but with random init

  # Regexp (re.compile) patterns matching parameters to be frozen.
  freeze_params:
    # # Frozen LLM
    # - "^llm\\..+$"  # LLM
    # - "^embed_tokens\\..+$"  # LLM embedding is moved
    # Frozen pretrained ASR (only the modality adapter layers are trainable)
    - "^perception\\.preprocessor\\..+$"
    - "^perception\\.encoder\\..+$"
  prevent_freeze_params: []  # Use to make specific submodules trainable; overrides freeze_params

  prompt_format: llama2
  audio_locator_tag: "<|audioplaceholder|>"  # placeholder token for audio turn is expected
  input_roles: ["user", "User"]
  output_roles: ["agent", "Assistant"]
  # Note: Uncomment the block below to enable LoRA on LLM via HuggingFace PEFT library.
  #   It will automatically freeze LLM parameters even if freeze_params was unused,
  #   and prevent freezing any parameter that has the string '.lora_' in its name.
  # lora:
  #   task_type: CAUSAL_LM
  #   r: 8
  #   lora_alpha: 32
  #   lora_dropout: 0.1

  # For Canary-1B baseline
  # perception:
  #    target: nemo.collections.speechlm2.modules.perception.AudioPerceptionModule
  #    modality_adapter:
  #      _target_: nemo.collections.asr.modules.ConformerEncoder
  #      feat_in: 1024
  #      feat_out: -1 # you may set it if you need different output size other than the default d_model
  #      n_layers: 2
  #      d_model: 1024
  #      subsampling: dw_striding # vggnet, striding, stacking or stacking_norm, dw_striding
  #      subsampling_factor: 1 # must be power of 2 for striding and vggnet
  #      subsampling_conv_channels: 256 # set to -1 to make it equal to the d_model
  #      causal_downsampling: false
  #      ff_expansion_factor: 4
  #      self_attention_model: rel_pos # rel_pos or abs_pos
  #      n_heads: 8 # may need to be lower for smaller d_models
  #      # [left, right] specifies the number of steps to be seen from left and right of each step in self-attention
  #      att_context_size: [-1, -1]
  #      att_context_style: regular # regular or chunked_limited
  #      xscaling: true # scales up the input embeddings by sqrt(d_model)
  #      untie_biases: true # unties the biases of the TransformerXL layers
  #      pos_emb_max_len: 5000
  #      conv_kernel_size: 9
  #      conv_norm_type: batch_norm # batch_norm or layer_norm or groupnormN (N specifies the number of groups)
  #      # conv_context_size can be"causal" or a list of two integers while conv_context_size[0]+conv_context_size[1]+1==conv_kernel_size
  #      # null means [(kernel_size-1)//2, (kernel_size-1)//2], and 'causal' means [(kernel_size-1), 0]
  #      conv_context_size: null
  #      ### regularization
  #      dropout: 0 # The dropout used in most of the Conformer Modules
  #      dropout_pre_encoder: 0 # The dropout used before the encoder
  #      dropout_emb: 0.0 # The dropout used for embeddings
  #      dropout_att: 0 # The dropout for multi-headed attention modules

  perception:
     target:  nemo.collections.speechlm2.modules.perception.AudioPerceptionModule
     modality_adapter:
       _target_: nemo.collections.asr.modules.ConformerEncoder
       feat_in: 512
       feat_out: -1 # you may set it if you need different output size other than the default d_model
       n_layers: 2
       d_model: 512
       subsampling: dw_striding # vggnet, striding, stacking or stacking_norm, dw_striding
       subsampling_factor: 1 # must be power of 2 for striding and vggnet
       subsampling_conv_channels: 256 # set to -1 to make it equal to the d_model
       causal_downsampling: true
       ff_expansion_factor: 4
       self_attention_model: rel_pos # rel_pos or abs_pos
       n_heads: 8 # may need to be lower for smaller d_models
       # [left, right] specifies the number of steps to be seen from left and right of each step in self-attention
       att_context_size: [70, 1] # -1 means unlimited context
       att_context_style: chunked_limited # regular or chunked_limited
       xscaling: true # scales up the input embeddings by sqrt(d_model)
       untie_biases: true # unties the biases of the TransformerXL layers
       pos_emb_max_len: 5000
       conv_kernel_size: 9
       conv_norm_type: layer_norm # batch_norm or layer_norm or groupnormN (N specifies the number of groups)
       # conv_context_size can be"causal" or a list of two integers while conv_context_size[0]+conv_context_size[1]+1==conv_kernel_size
       # null means [(kernel_size-1)//2, (kernel_size-1)//2], and 'causal' means [(kernel_size-1), 0]
       conv_context_size: causal
       ### regularization
       dropout: 0 # The dropout used in most of the Conformer Modules
       dropout_pre_encoder: 0 # The dropout used before the encoder
       dropout_emb: 0.0 # The dropout used for embeddings
       dropout_att: 0 # The dropout for multi-headed attention modules

  optimizer:
    _target_: torch.optim.AdamW
    lr: 3e-4
    betas: [0.9, 0.98]
    weight_decay: 1e-3
    foreach: true # set to false if having issues with tensor-parallelism

  lr_scheduler:
    _target_: nemo.core.optim.lr_scheduler.CosineAnnealing
    warmup_steps: 0
    min_lr: 1e-6
    max_steps: ${trainer.max_steps}

trainer:
  devices: -1
  accelerator: gpu
  num_nodes: 1
  precision: bf16-true
  logger: False # logger provided by exp_manager
  enable_checkpointing: False
  use_distributed_sampler: False
  max_steps: 100000
  limit_train_batches: 100  # "epoch" size
  val_check_interval: ${trainer.limit_train_batches}
  limit_val_batches: 10
  log_every_n_steps: 10
  num_sanity_val_steps: 1
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  strategy:
    # Replace DDPStrategy with ModelParallelStrategy to enable model parallelism
    _target_: lightning.pytorch.strategies.DDPStrategy
    gradient_as_bucket_view: true
    find_unused_parameters: true
    # _target_: lightning.pytorch.strategies.ModelParallelStrategy
    # tensor_parallel_size: 1
    # data_parallel_size: 2

data:
  train_ds:
    sample_rate: 16000
    prompt_format: ${model.prompt_format}
    token_equivalent_duration: 0.08
    input_cfg:
      - type: s2s_as_conversation
        # input_cfg: /lustre/fsw/portfolios/convai/users/apasad/nemo_duplex_s2s/configs/data_blends/salm_train_s2s_toy_dfw.yaml
        # cuts_path: ???  # needs to be set
        input_cfg: /lustre/fsw/portfolios/convai/users/cchen1/duplex_s2s/train_all.yaml
        audio_locator_tag: ${model.audio_locator_tag}
        input_roles: ${model.input_roles}
        output_roles: ${model.output_roles}
    seed: 42
    shuffle: true
    shard_seed: "randomized"
    num_workers: 1
    # batch_size: 16
    # Optional bucketing:
    batch_size: null
    batch_duration: null
    max_duration: null
    # batch_duration: 600
    # bucket_duration_bins: [8.94766,10.1551,11.64118,19.30376,42.85]
    # max_duration: 199.36
    # num_buckets: 5
    # bucket_duration_bins: [18.69587,24.24477,27.98826,31.22068,34.20082,37.25114,40.238,43.23646,46.3995,49.47955,52.04722,54.9649,58.45705,61.80676,65.17624,68.47347,71.63179,74.81107,78.17941,81.47664,85.93488,91.41478,97.45197,103.51918,109.99075,118.72825,127.20354,133.95601,140.90422]
    # num_buckets: 30
    # bucket_duration_bins in number of tokens
    batch_tokens: 2000
    bucket_duration_bins: [64, 128, 256, 384, 512, 768, 1024, 1280, 1536, 2048]
    max_tokens: 2048
    use_bucketing: true
    num_buckets: 10
    bucket_buffer_size: 5000
    use_multimodal_sampling: true

  # validation_ds:
  #   # The entries under 'datasets' are a list of separate dataloaders.
  #   # The structure is <dataset-name>: {<dataloader-dict-config>}
  #   # They inherit all settings from validation_ds, but can individually override them.
  #   prompt_format: ${model.prompt_format}
  #   token_equivalent_duration: 0.08
  #   datasets:
  #     val_set_0:  # rename to your dataset name, add more as needed
  #       input_cfg:
  #         - type: s2s_as_conversation
  #           input_cfg: /lustre/fsw/portfolios/convai/users/apasad/nemo_duplex_s2s/configs/data_blends/salm_train_s2s_toy_dfw.yaml
  #           # cuts_path: ???  # needs to be set
  #           audio_locator_tag: ${model.audio_locator_tag}
  #           input_roles: ${model.input_roles}
  #           output_roles: ${model.output_roles}
  #   sample_rate: 16000
  #   batch_size: 1
  #   seed: 42
  #   shard_seed: "randomized"

  validation_ds:
    # The entries under 'datasets' are a list of separate dataloaders.
    # The structure is <dataset-name>: {<dataloader-dict-config>}
    # They inherit all settings from validation_ds, but can individually override them.
    prompt_format: ${model.prompt_format}
    token_equivalent_duration: 0.08
    datasets:
      # nvolve_chatqa_test:
      #   input_cfg:
      #     - type: s2s_as_conversation
      #       input_cfg:
      #         - type: lhotse_shar
      #           shar_path: /lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_speechlm/data/duplex/nvolve_chatqa_test/
      #       audio_locator_tag: ${model.audio_locator_tag}
      #       input_roles: ${model.input_roles}
      #       output_roles: ${model.output_roles}
      # nvolve_multiturn_test:
      #   input_cfg:
      #     - type: s2s_as_conversation
      #       input_cfg:
      #         - type: lhotse_shar
      #           shar_path: /lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_speechlm/data/duplex/brainy_mantis/emma_trimmed_overlap_0.64/nvolve_multiturn_test/
      #       audio_locator_tag: ${model.audio_locator_tag}
      #       input_roles: ${model.input_roles}
      #       output_roles: ${model.output_roles}
      # 340b_8.19_daring_anteater_lmsys_sft_8801rm3.7p_test:
      #   input_cfg:
      #     - type: s2s_as_conversation
      #       input_cfg:
      #         - type: lhotse_shar
      #           shar_path: /lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_speechlm/data/duplex/340b_8.19_daring_anteater_lmsys_sft_8801rm3.7p_test/
      #       audio_locator_tag: ${model.audio_locator_tag}
      #       input_roles: ${model.input_roles}
      #       output_roles: ${model.output_roles}
      ultrachat_v2:
        input_cfg:
          - type: s2s_as_conversation
            input_cfg:
              - type: lhotse_shar
                shar_path: /lustre/fsw/portfolios/convai/projects/convai_convaird_nemo-speech/data/duplex/kevinhu/ultrachat_v2/shar_duplex/manifest_000200/
            audio_locator_tag: ${model.audio_locator_tag}
            input_roles: ${model.input_roles}
            output_roles: ${model.output_roles}
      topic_v2_llama3:
        input_cfg:
          - type: s2s_as_conversation
            input_cfg:
              - type: lhotse_shar
                shar_path: /lustre/fsw/portfolios/convai/projects/convai_convaird_nemo-speech/data/duplex/kevinhu/topic_v2/Meta-Llama-3.1-70B-Instruct/shar_duplex/manifest_000050
            audio_locator_tag: ${model.audio_locator_tag}
            input_roles: ${model.input_roles}
            output_roles: ${model.output_roles}
      tqa_nocontext:
        input_cfg:
          - type: s2s_as_conversation
            input_cfg:
              - type: lhotse_shar
                shar_path: /lustre/fsw/portfolios/convai/projects/convai_convaird_nemo-speech/data/duplex/kevinhu/triviaqa_validation/shar_duplex/manifest_000000/
            audio_locator_tag: ${model.audio_locator_tag}
            input_roles: ${model.input_roles}
            output_roles: ${model.output_roles}
      tqa_context40:
        input_cfg:
          - type: s2s_as_conversation
            input_cfg:
              - type: lhotse_shar
                shar_path: /lustre/fsw/portfolios/convai/projects/convai_convaird_nemo-speech/data/duplex/kevinhu/triviaqa_context40/shar_duplex/manifest_000000/
            audio_locator_tag: ${model.audio_locator_tag}
            input_roles: ${model.input_roles}
            output_roles: ${model.output_roles}
      llamaqa:
        input_cfg:
          - type: s2s_as_conversation
            input_cfg:
              - type: lhotse_shar
                shar_path: /lustre/fsw/portfolios/convai/projects/convai_convaird_nemo-speech/data/duplex/llama_qa/
            audio_locator_tag: ${model.audio_locator_tag}
            input_roles: ${model.input_roles}
            output_roles: ${model.output_roles}
      roleplay_v2:
        input_cfg:
          - type: s2s_as_conversation
            input_cfg:
              - type: lhotse_shar
                shar_path: /lustre/fsw/portfolios/convai/projects/convai_convaird_nemo-speech/data/duplex/synthetic_roleplay_v2_test/
            audio_locator_tag: ${model.audio_locator_tag}
            input_roles: ${model.input_roles}
            output_roles: ${model.output_roles}
    sample_rate: 16000
    batch_size: 1
    seed: 42
    shard_seed: "randomized"

exp_manager:
   exp_dir: null
   explicit_log_dir: salm_results/
   name: salm
   create_tensorboard_logger: false
   create_checkpoint_callback: true
   use_datetime_version: true
   max_time_per_run: 00:03:50:00

   resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.
   # you need to set these two to True to continue the training
   resume_if_exists: true
   resume_ignore_no_checkpoint: true

   # You may use this section to create a W&B logger
   create_wandb_logger: true
   wandb_logger_kwargs:
     name: development-run
     project: salm
     resume: true

   checkpoint_callback_params:
     filename: "{step}"
     monitor: val_acc
     mode: max
     every_n_train_steps: null
     every_n_epochs: 1
     save_top_k: 1
     always_save_nemo: false
     save_nemo_on_train_end: false